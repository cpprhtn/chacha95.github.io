---
layout: post
title: Softmax Classification
tags: [ML]
---

# Softmax Classfication이란?

[이전포스트](<https://chacha95.github.io/2018-11-05-Regression/>) 에서는 두개의 클래스에 대해 분류하는 법(binary classfication)을 배웠습니다. 

이번 포스트에서는 Softmax에 대해 알아봅시다.

>  다음과 같이 분류하고자 하는 클래스가 여러개 일땐 어쩌죠?

<center><img src="https://user-images.githubusercontent.com/31475037/59080004-88967a80-8921-11e9-86fb-5d6b918520a8.png" width="90%"></center>
Softmax 함수는 함수에 들어오는 값들을 확률값으로 바꿔주는  기능을 합니다.

이를 통해 우리는 모델을 통해 예측한 결과값을 확률값으로 바꿔서 볼 수 있습니다.(sigmoid와 같은 기능을 함)

각 훈련 데이터가 [x1, x2, x3]의 벡터로 표현된다고 해봅시다.

> weight 벡터를 모아 행렬의 형태로 표현

<center><img src="https://user-images.githubusercontent.com/31475037/59089563-71698400-8945-11e9-8bd5-e10655ba5156.png"></center>
> matrix multiplication을 통해 결과값을 도출해낸다.

<center><img src="https://user-images.githubusercontent.com/31475037/59080008-892f1100-8921-11e9-984f-25ef0391bdc7.png"></center>
이후 소프트 맥스를 통해 결과값(scores)을 확률값(probability)으로 바꿔줍니다.

> Softmax

Softmax함수를 통과한 출력값은 0~1범위의 확률값이며 모든 확률의 합을 1로 만듭니다.

<center><img src="https://user-images.githubusercontent.com/31475037/59080010-892f1100-8921-11e9-881c-78d92391672f.png" width="80%"></center>
> 이후 나온 확률값을 one-hot encoding을 통해 가장 높은 확률을 가진 클래스를 선택

one-hot encoding은 softmax를 통해 나온 확률값 중에 가장 높은 클래스를 선택해 해당 클래스만 1로 만들고 나머지는 다 0으로 만들어줍니다.

<center><img src="https://user-images.githubusercontent.com/31475037/59080011-89c7a780-8921-11e9-848d-764e370d6f60.png" width="90%"></center>
우리가 만든 모델이, 행렬곱과 softmax 그리고 one-hot encoding과정을 거쳐 특정 클래스를 골랐네요.

그렇다면 모델이 고른것에대해 잘 골랐는지 아닌지 어떻게 평가를 할 것인가요?

<br>

## Cross-Entropy

모델이 예측한 클래스에대해서 지도학습이기에 라벨링(정답)이 되있습니다. 그렇기 때문에 모델이 예측한 클래스와 정답 클래스를 알기에 이를 기반으로 모델이 잘 예측했는지 아닌지에 대해서 Cost(Loss)함수를 정의해 평가를 해줍니다. 이번에 사용한 Cost 함수는 Cross-Entropy입니다.

> Cross-Entropy란?

Cross-Entropy는 다음 그림에서 나온것과 같이 표현됩니다.

Cross-Entorpy 함수에 대해서 이해하기 위해선 정보이론에 대해서 알아야합니다.

<center><img src="https://user-images.githubusercontent.com/31475037/59080012-89c7a780-8921-11e9-9e11-24dcd6c40af1.png" width="90%"></center>
**정보이론(Information Theory)**

정보이론은 다음과 같습니다.

* 정보(information)란 놀람의 정도를 수치화
* 확률 함수를 이용해 정의
* 정보량은 확률에 반비례
* 어떤 사람이 정보를 더 많이 알수록 새롭게 알 수 있는 정보는 적어짐
* 어떤 사건의 확률이 매우 높다고 가정했을 때, 그 사건이 발생해도 별로 놀라지 않는다.즉, 이 사건은 적은 정보를 제공한다(정보량이 낮다)
* 반면 사건의 확률이 매우 낮으면, 그 사건이 발생할시 놀랍다(정보량이 많다)

> 정보량 공식

<center><img src="https://user-images.githubusercontent.com/31475037/59091784-091da100-894b-11e9-8243-07be3a7c5c86.png" width="40%"></center>
> 로또에 당첨될 확률 (정보량이 많다)

<center><img src="https://user-images.githubusercontent.com/31475037/59080013-89c7a780-8921-11e9-96ed-8af662f10d0e.png" width="50%"></center>
**Entropy**

entropy는 이러한 정보량(놀람의 정도)들의 평균을 말합니다.

<center><img src="https://user-images.githubusercontent.com/31475037/59080014-89c7a780-8921-11e9-837e-14f1ccf50c39.png" width="50%"></center>
**KL-divergence(Kullbeack-Leibler 발산)**

두 확률 분포 p와 q가 얼마나 다른지 측정하는 방식입니다.

KL-divergence가 0에 가깝다는 의미는 두 확률 분포가 거의 비슷하다는 의미입니다.

개념상 두 확률 분포 사이의 distance와 비슷합니다.(하지만 실제 distance는 아님)

p는 ground_truth distribution, q는 예측한 distribution입니다.

<center><img src="https://user-images.githubusercontent.com/31475037/59080015-8a603e00-8921-11e9-9e79-b32abcf35ccf.png"></center>
KL divergence를 최소화하는것은 결국 첫번째 항 cross-entropy를 최소화하는 q를 찾는것입니다.

<center><img src="https://user-images.githubusercontent.com/31475037/59080017-8a603e00-8921-11e9-8092-ddd9b509377a.png" width="80%"></center>
<br>

## Softmax와 Sigmoid

그럼 앞에서 배운 Sigmoid와 Softmax의 특성을 비교해볼까요?

**Sigmoid**

수학적 정의에서 sigmoid 함수는 모든 범위의 실수를 취하고 0~1사이의 출력 값을 반환합니다.

주된 함수 사용은 다음과 같습니다.

* Logistic Regression에서 binary classification에 사용됨
* Neural Network에서 activation function으로 사용됨
* 통계에서 누적분포 함수로 사용됨



**Softmax**

Softmax함수는 'n'개의 다른 이벤트에 대해 이벤트의 확률 분포를 계산합니다.

일반적으로, 이 함수는 가능한 모든 대상 클래스에 대해 각 대상 클래스의 확률을 계산합니다. Softmax의 출력값은 0~1범위의 확률값이며 모든 확률의 합이 1입니다.

주된 함수 사용은 다음과 같습니다.

* multi-classfication에서 사용

<br>

[Cross-Entropy](https://curt-park.github.io/2018-09-19/loss-cross-entropy/)

[Softmax pytorch 구현 코드](https://github.com/chacha95/Deeplearning_example/blob/master/03.Softmax_Classification.ipynb)

<br>

