---
layout: post
title: Dimensionality Reduction(차원 감소)와 PCA
tags: [Math]
use_math: true
---

# Dimenstion(차원)

일반적으로 차원이란, 0차원, 1차원, 2차원 3차원 등으로 표현합니다. 기하학적으로 보면 0차원은 점의 세계, 1차원은 선의 세계, 2차원은 면의 세계, 3차원은 공간의 세계입니다. 보통 3차원까지는 머리 속으로 쉽게 상상이 가지만 4차원 이상부터는 잘 형상화가 되지 않습니다.

하지만 4차원도 형상화가 가능하며 이를 그림으로 나타내면 다음과 같습니다.

> 차원 증가에 대한 일반적인 개념, 기저 벡터가 늘어남에 따라 차원도 늘어남

<center><img src="https://user-images.githubusercontent.com/31475037/64427491-50533380-d0ec-11e9-98ab-6be9accc6072.png"></center>
특히나 차원이란 기저 벡터의 수로 결정되며, 벡터는 물리학, computer science, 수학에서 의미하는 바가 다릅니다. 물리학에선 속력과 방향을 가진 것으로 표현되고, computer science에서는 순서를 가진 순차 리스트 이며, 수학에선 이 두개를 포괄하는 개념으로 정의 됩니다.. 관련해서 [자세히 소개하는 영상](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)을 보시는걸 추천합니다.

우리가 주로 사용할 개념은 computer science에서 보는 관점인 순차 리스트로서의 벡터입니다. 

Computer science에서 이미지를 표현하는 방식에 대해 봅시다. 아래 두 장의 차원이 다른 사진을 보시겠습니다. 

왼쪽 그림은 크기가 120 x 80 (9,600 차원), 오른쪽 그림은 1200 x 800 (960,000 차원)으로 크기가 백배 차이가 납니다. 차원이 큰 사진이 용량도 100 배 더 크고, 정보도 훨씬 고밀도로 저장되어 있습니다.

아래의 그림은 차원에 따른 정보량의 차이에 대해 나타냅니다.

> 차원에 따른 정보량의 차이

<center><img src="https://user-images.githubusercontent.com/31475037/64427492-50ebca00-d0ec-11e9-9093-a63c9504e429.png"></center>
차원이 많을 수록 더 많은 정보량을 담을 수 있어 항상 좋아보입니다. 하지만 차원의 저주 때문에 항상 좋은 것만은 아닙니다.



## 차원의 저주(Curse of Dimensionality)

데이터의 차원이 증가할수록 해당 공간의 크기(부피)가 기하급수적으로 증가하기 때문에 동일한 개수의 데이터의 밀도는 차원이 증가할수록 급속도록 sparse(희소, 희박)해집니다. 또한 차원이 증가할수록 데이터의 분포 분석 또는 모델추정에 필요한 샘플 데이터의 개수가 기하급수적으로 증가하게 됩니다. 

따라서 고 차원에서의 데이터 분석이 어려워집니다.

> 차원이 높아짐에 따라 데이터의 밀도는 낮아짐, 이를 sparse해진다고 표현

<center><img src="https://user-images.githubusercontent.com/31475037/64427493-50ebca00-d0ec-11e9-981e-419102c83846.png" width="70%"></center>
저 차원에서 쓰이던 두 샘플 데이터간의 거리를 측정하는 기준들이 고차원에서는 사용이 힘듭니다. 

고차원 공간에서 유클리디언(Euclidean) 거리 상으로는 가까우나 실제로는 먼 경우가 생길 수 있습니다.

즉, 차원의 저주로 인해 고 차원에서의 유의미한 거리 측정 방식을 찾기 어렵습니다. 이러한 문제를 해결하기 위해 제시된 가설이 바로 매니폴드 가설(Manifold Hypothesis)입니다.

고 차원의 데이터는 밀도는 낮지만, 이들의 집합을 포함하는 저 차원의 매니폴드가 존재해 고차원의 데이터를 저 차원의 매니폴드로 차원을 낮춰도 데이터가 손실되지 않는다는 가설입니다. 즉, 고차원 데이터를 손실 없이 아우르는 subspace를 칭합니다.

매니폴드 영역에서의 이점은 다음과 같습니다.

- 데이터 압축 이 가능하다(압축된 데이터로부터 원본 이미지를 복원 할 수 있다.)
- 데이터 시각화이 손쉬워 진다
- 차원의 저주 해결
- 중요한 feature 를 찾을 수 있습니다.

고차원에서는 유의미한 거리 측정 방식을 찾기가 힘들기에 저 차원의 매니폴드 영역으로 차원을 축소해 유클리디언 거리 측정과 같은 방식을 이용합니다. 

매니폴드 영역에서는 고차원에서는 사용하기 어려웠던 유클리디언 거리측정과 같은 방식을 사용할 수 있습니다.
아래 그림의 고차원에서 점 A2 와 점 B 사이를 유클리디언 거리로 측정할 시 실제 거리와 비슷하게 나오지만, 점 A1 과 점 B 사이의 거리는 굉장히 머나 유클리디언 거리 상으로는 가깝게 나옵니다.

 하지만 매니폴드 영역에서의 거리를 측정하게 되면 실제 거리와 유클리디언 거리가 비슷하게 나옵니다.

> 고 차원과 manifold에서의 distance 측정

<center><img src="https://user-images.githubusercontent.com/31475037/64427495-50ebca00-d0ec-11e9-977f-0a06e0bd954d.png"></center>
이런 고차원의 데이터를 저 차원에 표현하는 기법을 차원 감소(Dimensionality Reduction)라고 부릅니다. 

그렇다면 어떻게 고차원의 공간을 저차원으로 projection(투영) 할 수 있을까요?

가장 대표적인 방법이 바로 PCA입니다.

<br>

# PCA(Principal Component Analysis)

PCA 는 데이터의 분산(variance)를 최대한 보존하면서 서로 직교하는 principal axis(주성분)를 찾아, 고차원 공간의 표본들을 저 차원 공간으로 변환하는 기법입니다. 즉 principal axis는 데이터의 분산을 최대한 보존하는 새 기저를 의미합니다.

그렇다면 어떻게 principal axis를 찾을까요? 

다음의 예시를 통해 알아봅시다. 

> 데이터를 원형으로 뿌려 놓았을 때

<center><img src="https://user-images.githubusercontent.com/31475037/64518271-d2ca3600-d32c-11e9-9567-eb50ac0b5c3a.PNG" width="70%"></center>
이 데이터의 점들에 대해 다음 행렬을 통해 선형 변환을 해봅시다.

행렬 **A**는 Covariance 행렬로 다음과 같이 구성됩니다.

> Covariance 행렬 **A**

$$
\textbf A = \begin{bmatrix}3 & 2\\2 & 4\\\end{bmatrix}
$$



여기서 Covariance 행렬은 서로 다른 변수들 사이에서 얼마나 의존 하는가에 대해 수치적으로 표현한 행렬입니다.

> Covariance 행렬

$$
\begin{bmatrix}
Var (X) & Cov(X, Y)\\
Cov(Y, X) & Var(Y)\\
\end{bmatrix}
$$

행렬 **A**의 1행 1열 원소는 1번 feature의 variance를 나타냅니다. 즉 x축 방향으로 얼마만큼 퍼지게 할 것인가를 말해줍니다. 1행 2열의 원소와 2행 1열의 원소는 각각 x, y축으로 함께 얼마만큼 퍼지게 할 것인가를 말해줍니다. 2행 2열의 원소는 y축 방향으로 얼마만큼 퍼지게 할 것인가를 말해줍니다.

> 행렬의 원소들의 의미

<center><img src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-07-27_PCA/pics_mtx1.png" width="80%"></center>
이 행렬에 의해 선형변환 된 후의 결과를 봅시다.

> 행렬에 의해 선형변환 된 후

<center><img src="https://user-images.githubusercontent.com/31475037/64518272-d362cc80-d32c-11e9-8723-66b652027704.PNG" width="70%"></center>
Covariance 행렬에서 eigenvaectors를 찾는 것은 데이터의 principal axis을 찾는 것과 동일합니다. 

Eigenvector의 의미를 잘 생각해보면, eigenvector는 선형변환시 vector의 방향이 변하지 않는 vector를 말합니다. 즉 해당 행렬의 특징을 잘 가지고 있는 vector 입니다. 그렇기에 principal axis가 eigenvector가 되게 됩니다.

여기서 Eigenvalues은 각 eigenvectors에 해당하는 correlation이라 볼 수 있습니다. 그리고 eigenvalue가 큰 순서대로 eigenvector를 정렬하면, 결과적으로 중요한(correlation이 높은) 순서대로 principal axis을 구하는 것이 됩니다.




> Covariance 행렬의 principal axis

<center><img src="https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2019-07-27_PCA/pics_eigen.png" width="70%"></center>
PCA 란 한마디로 말하면 입력 데이터들의 공분산 행렬에 대한 eigenvalue decomposition으로 볼 수 있습니다.
이 때 나오는 고유벡터가 주 성분 벡터로서 데이터의 분포에서 분산이 큰 방향을 나타내고, 대응되는 고유값이 그 분산의 크기를 나타냅니다. 이는 eigenvalue가 고유하며, 선형변환 시에도 크기만 변하고 가르키는 방향은 변하지 않는다는 특징을 활용한 것입니다.

<br>

## PCA 활용

### 차원 감소

차원 감소 기법 중에서 PCA 는 전통적으로 대표적인 기법 중 하나입니다.

아래 그림에서와 같이 3 차원에서 분포하는 데이터에 대해 2 차원으로 차원 감소를 할 때, 분산을 최대한 유지시키도록 projection 함으로, 3 차원에서의 데이터 분포를 최대한 보존하게 2 차원으로 projection 합니다.

<center><img src="https://user-images.githubusercontent.com/31475037/64427499-51846080-d0ec-11e9-9055-01619d071717.png"></center>
### Eigenface와 영상인식

앞서 PCA 를 통해 얻어진 principal axis들은 서로 수직인 관계에 있습니다. 이 말은 주성분 벡터들이 n 차원 공간을 생성하는 기저 역할을 할 수 있음을 의미합니다. 

즉, PCA 로 얻은 주성분 벡터들은 선형 결합으로 표현될 수 있습니다. 그런데 뒷부분의 principal axis들은 데이터 분포에 포함된 노이즈성 정보를 나타내기 때문에 뒷부분은 버리고 전반부 k 개의 주성분 벡터들만을 가지고 원래 데이터를 표현하면 노이즈가 제거된 데이터를 얻을 수 있습니다. 

> k 값에 따른 eigenface의 변화

<center><img src="https://user-images.githubusercontent.com/31475037/64427500-51846080-d0ec-11e9-8664-76353682751d.png"></center>
그림에서 볼 수 있듯이 많은 수의 eigenface 를 이용하면 원본 얼굴과 거의 유사한 근사(복원)결과를 볼 수 있지만 k 가 작아질수록 고유의 얼굴 특성은 사라지고 공통의 얼굴 특성이 남게 됩니다. (k=20 인 경우 원래 얼굴이 그대로 살아나지만 k=2 인 경우 공통적인 부분만 남음)

### PCA 활용 정리

첫째 차원 감소의 역할 : n 차원의 데이터를 새로운 좌표축으로 proejction, 이를 통해 차원을 감소시킵니다.

둘째 데이터 압축의 역할 : 원본 그대로를 저장하지 않고 k개의 주성분들과 계수들만을 저장하면 저장용량을 크게 줄 일수 있다는 의미입니다.
마지막으로 노이즈 제거의 의미는 k 개의 주성분만을 이용해서 데이터를 복원함으로써 의미 없는 노이즈 부분을 날린다는 의미입니다.

<br>

**참고 강의**

[오토 인코더의 모든 것](https://d2.naver.com/news/0956269)

[3Blue1Brown](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)

[다크 프로그래머 PCA](https://darkpgmr.tistory.com/110)

[공돌이의 정리 노트](https://angeloyeo.github.io/2019/07/27/PCA.html)